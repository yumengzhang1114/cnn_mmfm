{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lifelines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBWaKcCzqC_l",
        "outputId": "54907dc6-645c-4de8-91ad-f0f39a604cb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lifelines\n",
            "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.15.3)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.8.0)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines)\n",
            "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n",
            "Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=35754fc944f4534b7192e6f711b21f9741ab1884e95524144a46688199572414\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: interface-meta, autograd-gamma, formulaic, lifelines\n",
            "Successfully installed autograd-gamma-0.5.0 formulaic-1.1.1 interface-meta-1.3.0 lifelines-0.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i5A0JRw1zh1T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from lifelines.utils import concordance_index\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi_DtAiKzoOd",
        "outputId": "cc39ac78-0589-4606-93f7-631cfb9a9640"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {DEVICE}\")\n",
        "\n",
        "# Paths and column definitions\n",
        "SURVIVAL_DF_PATH = '/content/drive/MyDrive/surv.csv'\n",
        "IMAGING_PKL_PATH = '/content/drive/MyDrive/x_with_coord.pkl'\n",
        "TABULAR_COLS = [\"AGE\", \"GENDER\", \"PTEDUCAT\", \"APOE4\"]\n",
        "PATIENT_ID_COL = \"ID\"\n",
        "TIME_COL, EVENT_COL, GROUP_COL = \"Years_bl\", \"status\", PATIENT_ID_COL\n",
        "\n",
        "##Image Data Loading and Pre-processing\n",
        "imaging_df = pd.read_pickle(IMAGING_PKL_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWlndGqzzoQ4",
        "outputId": "6715eac8-b550-40b8-b11e-a1db65ac07a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = imaging_df.copy()\n",
        "block = 2\n",
        "signal_cols = df.columns[3:]\n",
        "\n",
        "df[\"Xb\"] = df.x // block\n",
        "df[\"Yb\"] = df.y // block\n",
        "df[\"Zb\"] = df.z // block\n",
        "\n",
        "print(\"Mean‑pooling …\")\n",
        "pooled = df.groupby([\"Xb\", \"Yb\", \"Zb\"])[signal_cols].mean()\n",
        "\n",
        "pooled = pooled.reset_index()\n",
        "pooled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "iFYy8hBy_TIT",
        "outputId": "068e8816-cc16-45dc-d1ae-728c6792dd31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean‑pooling …\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Xb  Yb  Zb  002_S_0729_2006-07-17_5.csv  002_S_0782_2006-08-14_3.csv  \\\n",
              "0        1  37  44                    -2.792912                    -2.378889   \n",
              "1        1  37  45                    -2.808298                    -2.378889   \n",
              "2        1  37  46                    -2.789328                    -2.378889   \n",
              "3        1  38  43                    -2.791528                    -2.375753   \n",
              "4        1  38  44                    -0.863732                    -2.348688   \n",
              "...     ..  ..  ..                          ...                          ...   \n",
              "239163  77  49  50                    -2.728616                    -2.360885   \n",
              "239164  77  49  51                    -2.726615                    -2.380032   \n",
              "239165  77  49  52                    -2.722288                    -2.413499   \n",
              "239166  77  51  40                    -2.762474                    -2.382130   \n",
              "239167  77  52  40                    -2.764746                    -2.379925   \n",
              "\n",
              "        002_S_0954_2006-10-10_7.csv  002_S_1070_2006-11-28_11.csv  \\\n",
              "0                         -1.773856                     -2.335286   \n",
              "1                         -1.773856                     -2.329250   \n",
              "2                         -1.773856                     -2.329084   \n",
              "3                         -1.773856                     -2.517997   \n",
              "4                         -1.773856                     -2.496181   \n",
              "...                             ...                           ...   \n",
              "239163                    -1.773856                     -2.321014   \n",
              "239164                    -1.773856                     -2.313229   \n",
              "239165                    -1.773856                     -2.313831   \n",
              "239166                    -1.773856                     -2.328615   \n",
              "239167                    -1.773856                     -2.328613   \n",
              "\n",
              "        002_S_1155_2006-12-14_9.csv  002_S_1268_2007-02-14_11.csv  \\\n",
              "0                          0.596719                     -1.765281   \n",
              "1                          0.901965                     -1.765281   \n",
              "2                          0.651638                     -1.765281   \n",
              "3                          0.632981                     -1.765281   \n",
              "4                          1.103007                     -1.765281   \n",
              "...                             ...                           ...   \n",
              "239163                    -2.579449                     -1.765281   \n",
              "239164                    -2.566476                     -1.765281   \n",
              "239165                    -2.569669                     -1.765281   \n",
              "239166                    -2.588504                     -1.765231   \n",
              "239167                    -2.588504                     -1.765160   \n",
              "\n",
              "        002_S_2010_2010-06-24_11.csv  ...  941_S_2060_2010-09-08_733.csv  \\\n",
              "0                          -2.833100  ...                      -1.701152   \n",
              "1                          -2.829968  ...                      -1.701152   \n",
              "2                          -2.832926  ...                      -1.701152   \n",
              "3                          -2.769821  ...                      -1.701152   \n",
              "4                          -2.746532  ...                      -1.701152   \n",
              "...                              ...  ...                            ...   \n",
              "239163                     -2.817723  ...                      -1.761741   \n",
              "239164                     -2.826856  ...                      -1.780595   \n",
              "239165                     -2.828936  ...                      -1.737624   \n",
              "239166                     -2.810227  ...                      -1.701075   \n",
              "239167                     -2.821986  ...                      -1.700793   \n",
              "\n",
              "        941_S_4036_2011-05-10_735.csv  941_S_4187_2011-06-22_734.csv  \\\n",
              "0                            0.875000                       0.201460   \n",
              "1                            0.155349                       0.600329   \n",
              "2                           -1.182283                       0.757916   \n",
              "3                            1.011001                       0.763914   \n",
              "4                            0.858369                       0.688716   \n",
              "...                               ...                            ...   \n",
              "239163                      -2.519014                      -2.863345   \n",
              "239164                      -2.517990                      -2.874205   \n",
              "239165                      -2.516753                      -2.876511   \n",
              "239166                      -2.520640                      -2.848475   \n",
              "239167                      -2.530971                      -2.855019   \n",
              "\n",
              "        941_S_4377_2012-01-04_740.csv  941_S_4420_2012-03-28_739.csv  \\\n",
              "0                           -2.583830                      -2.595766   \n",
              "1                           -2.571290                      -2.511538   \n",
              "2                           -2.688458                      -2.369983   \n",
              "3                           -0.858805                      -2.498166   \n",
              "4                           -1.114168                      -2.000347   \n",
              "...                               ...                            ...   \n",
              "239163                      -2.497635                      -2.421602   \n",
              "239164                      -2.528827                      -2.411746   \n",
              "239165                      -2.547080                      -2.399212   \n",
              "239166                      -2.551446                      -2.426327   \n",
              "239167                      -2.567371                      -2.411177   \n",
              "\n",
              "        941_S_4764_2012-06-01_742.csv  941_S_6017_2017-05-17_822.csv  \\\n",
              "0                           -1.767830                      -2.355316   \n",
              "1                           -1.328341                      -2.355316   \n",
              "2                           -2.284303                      -2.355316   \n",
              "3                           -2.478895                      -2.352305   \n",
              "4                           -0.240278                      -2.330434   \n",
              "...                               ...                            ...   \n",
              "239163                      -2.173714                      -2.367502   \n",
              "239164                      -2.184370                      -2.366173   \n",
              "239165                      -2.199603                      -2.369792   \n",
              "239166                      -2.171802                      -2.352401   \n",
              "239167                      -2.176203                      -2.339786   \n",
              "\n",
              "        941_S_6052_2017-07-20_825.csv  941_S_6068_2017-08-21_831.csv  \\\n",
              "0                            0.226981                       1.646702   \n",
              "1                            0.690027                       1.329351   \n",
              "2                           -0.780878                       1.504705   \n",
              "3                            1.436756                       1.877898   \n",
              "4                            1.394198                       1.328678   \n",
              "...                               ...                            ...   \n",
              "239163                      -2.477654                      -2.565042   \n",
              "239164                      -2.471008                      -2.574568   \n",
              "239165                      -2.468890                      -2.563396   \n",
              "239166                      -2.462863                      -2.548314   \n",
              "239167                      -2.462120                      -2.616969   \n",
              "\n",
              "        941_S_6345_2018-05-10_839.csv  \n",
              "0                           -2.352754  \n",
              "1                           -2.352754  \n",
              "2                           -2.352754  \n",
              "3                           -2.345762  \n",
              "4                           -2.335502  \n",
              "...                               ...  \n",
              "239163                      -2.344331  \n",
              "239164                      -2.326821  \n",
              "239165                      -2.334988  \n",
              "239166                      -2.340197  \n",
              "239167                      -2.317459  \n",
              "\n",
              "[239168 rows x 745 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe0261fb-1f63-4e2d-a981-7290c902bd52\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Xb</th>\n",
              "      <th>Yb</th>\n",
              "      <th>Zb</th>\n",
              "      <th>002_S_0729_2006-07-17_5.csv</th>\n",
              "      <th>002_S_0782_2006-08-14_3.csv</th>\n",
              "      <th>002_S_0954_2006-10-10_7.csv</th>\n",
              "      <th>002_S_1070_2006-11-28_11.csv</th>\n",
              "      <th>002_S_1155_2006-12-14_9.csv</th>\n",
              "      <th>002_S_1268_2007-02-14_11.csv</th>\n",
              "      <th>002_S_2010_2010-06-24_11.csv</th>\n",
              "      <th>...</th>\n",
              "      <th>941_S_2060_2010-09-08_733.csv</th>\n",
              "      <th>941_S_4036_2011-05-10_735.csv</th>\n",
              "      <th>941_S_4187_2011-06-22_734.csv</th>\n",
              "      <th>941_S_4377_2012-01-04_740.csv</th>\n",
              "      <th>941_S_4420_2012-03-28_739.csv</th>\n",
              "      <th>941_S_4764_2012-06-01_742.csv</th>\n",
              "      <th>941_S_6017_2017-05-17_822.csv</th>\n",
              "      <th>941_S_6052_2017-07-20_825.csv</th>\n",
              "      <th>941_S_6068_2017-08-21_831.csv</th>\n",
              "      <th>941_S_6345_2018-05-10_839.csv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>44</td>\n",
              "      <td>-2.792912</td>\n",
              "      <td>-2.378889</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.335286</td>\n",
              "      <td>0.596719</td>\n",
              "      <td>-1.765281</td>\n",
              "      <td>-2.833100</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.701152</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.201460</td>\n",
              "      <td>-2.583830</td>\n",
              "      <td>-2.595766</td>\n",
              "      <td>-1.767830</td>\n",
              "      <td>-2.355316</td>\n",
              "      <td>0.226981</td>\n",
              "      <td>1.646702</td>\n",
              "      <td>-2.352754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>45</td>\n",
              "      <td>-2.808298</td>\n",
              "      <td>-2.378889</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.329250</td>\n",
              "      <td>0.901965</td>\n",
              "      <td>-1.765281</td>\n",
              "      <td>-2.829968</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.701152</td>\n",
              "      <td>0.155349</td>\n",
              "      <td>0.600329</td>\n",
              "      <td>-2.571290</td>\n",
              "      <td>-2.511538</td>\n",
              "      <td>-1.328341</td>\n",
              "      <td>-2.355316</td>\n",
              "      <td>0.690027</td>\n",
              "      <td>1.329351</td>\n",
              "      <td>-2.352754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>46</td>\n",
              "      <td>-2.789328</td>\n",
              "      <td>-2.378889</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.329084</td>\n",
              "      <td>0.651638</td>\n",
              "      <td>-1.765281</td>\n",
              "      <td>-2.832926</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.701152</td>\n",
              "      <td>-1.182283</td>\n",
              "      <td>0.757916</td>\n",
              "      <td>-2.688458</td>\n",
              "      <td>-2.369983</td>\n",
              "      <td>-2.284303</td>\n",
              "      <td>-2.355316</td>\n",
              "      <td>-0.780878</td>\n",
              "      <td>1.504705</td>\n",
              "      <td>-2.352754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>43</td>\n",
              "      <td>-2.791528</td>\n",
              "      <td>-2.375753</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.517997</td>\n",
              "      <td>0.632981</td>\n",
              "      <td>-1.765281</td>\n",
              "      <td>-2.769821</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.701152</td>\n",
              "      <td>1.011001</td>\n",
              "      <td>0.763914</td>\n",
              "      <td>-0.858805</td>\n",
              "      <td>-2.498166</td>\n",
              "      <td>-2.478895</td>\n",
              "      <td>-2.352305</td>\n",
              "      <td>1.436756</td>\n",
              "      <td>1.877898</td>\n",
              "      <td>-2.345762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>44</td>\n",
              "      <td>-0.863732</td>\n",
              "      <td>-2.348688</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.496181</td>\n",
              "      <td>1.103007</td>\n",
              "      <td>-1.765281</td>\n",
              "      <td>-2.746532</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.701152</td>\n",
              "      <td>0.858369</td>\n",
              "      <td>0.688716</td>\n",
              "      <td>-1.114168</td>\n",
              "      <td>-2.000347</td>\n",
              "      <td>-0.240278</td>\n",
              "      <td>-2.330434</td>\n",
              "      <td>1.394198</td>\n",
              "      <td>1.328678</td>\n",
              "      <td>-2.335502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239163</th>\n",
              "      <td>77</td>\n",
              "      <td>49</td>\n",
              "      <td>50</td>\n",
              "      <td>-2.728616</td>\n",
              "      <td>-2.360885</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.321014</td>\n",
              "      <td>-2.579449</td>\n",
              "      <td>-1.765281</td>\n",
              "      <td>-2.817723</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.761741</td>\n",
              "      <td>-2.519014</td>\n",
              "      <td>-2.863345</td>\n",
              "      <td>-2.497635</td>\n",
              "      <td>-2.421602</td>\n",
              "      <td>-2.173714</td>\n",
              "      <td>-2.367502</td>\n",
              "      <td>-2.477654</td>\n",
              "      <td>-2.565042</td>\n",
              "      <td>-2.344331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239164</th>\n",
              "      <td>77</td>\n",
              "      <td>49</td>\n",
              "      <td>51</td>\n",
              "      <td>-2.726615</td>\n",
              "      <td>-2.380032</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.313229</td>\n",
              "      <td>-2.566476</td>\n",
              "      <td>-1.765281</td>\n",
              "      <td>-2.826856</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.780595</td>\n",
              "      <td>-2.517990</td>\n",
              "      <td>-2.874205</td>\n",
              "      <td>-2.528827</td>\n",
              "      <td>-2.411746</td>\n",
              "      <td>-2.184370</td>\n",
              "      <td>-2.366173</td>\n",
              "      <td>-2.471008</td>\n",
              "      <td>-2.574568</td>\n",
              "      <td>-2.326821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239165</th>\n",
              "      <td>77</td>\n",
              "      <td>49</td>\n",
              "      <td>52</td>\n",
              "      <td>-2.722288</td>\n",
              "      <td>-2.413499</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.313831</td>\n",
              "      <td>-2.569669</td>\n",
              "      <td>-1.765281</td>\n",
              "      <td>-2.828936</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.737624</td>\n",
              "      <td>-2.516753</td>\n",
              "      <td>-2.876511</td>\n",
              "      <td>-2.547080</td>\n",
              "      <td>-2.399212</td>\n",
              "      <td>-2.199603</td>\n",
              "      <td>-2.369792</td>\n",
              "      <td>-2.468890</td>\n",
              "      <td>-2.563396</td>\n",
              "      <td>-2.334988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239166</th>\n",
              "      <td>77</td>\n",
              "      <td>51</td>\n",
              "      <td>40</td>\n",
              "      <td>-2.762474</td>\n",
              "      <td>-2.382130</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.328615</td>\n",
              "      <td>-2.588504</td>\n",
              "      <td>-1.765231</td>\n",
              "      <td>-2.810227</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.701075</td>\n",
              "      <td>-2.520640</td>\n",
              "      <td>-2.848475</td>\n",
              "      <td>-2.551446</td>\n",
              "      <td>-2.426327</td>\n",
              "      <td>-2.171802</td>\n",
              "      <td>-2.352401</td>\n",
              "      <td>-2.462863</td>\n",
              "      <td>-2.548314</td>\n",
              "      <td>-2.340197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239167</th>\n",
              "      <td>77</td>\n",
              "      <td>52</td>\n",
              "      <td>40</td>\n",
              "      <td>-2.764746</td>\n",
              "      <td>-2.379925</td>\n",
              "      <td>-1.773856</td>\n",
              "      <td>-2.328613</td>\n",
              "      <td>-2.588504</td>\n",
              "      <td>-1.765160</td>\n",
              "      <td>-2.821986</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.700793</td>\n",
              "      <td>-2.530971</td>\n",
              "      <td>-2.855019</td>\n",
              "      <td>-2.567371</td>\n",
              "      <td>-2.411177</td>\n",
              "      <td>-2.176203</td>\n",
              "      <td>-2.339786</td>\n",
              "      <td>-2.462120</td>\n",
              "      <td>-2.616969</td>\n",
              "      <td>-2.317459</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>239168 rows × 745 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe0261fb-1f63-4e2d-a981-7290c902bd52')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe0261fb-1f63-4e2d-a981-7290c902bd52 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe0261fb-1f63-4e2d-a981-7290c902bd52');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d59355a3-2e92-4894-9892-1481a1b31415\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d59355a3-2e92-4894-9892-1481a1b31415')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d59355a3-2e92-4894-9892-1481a1b31415 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_bb768ab1-6783-4c40-922c-21c07cb56c97\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pooled')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bb768ab1-6783-4c40-922c-21c07cb56c97 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pooled');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pooled"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SurvivalCNN(nn.Module):\n",
        "    def __init__(self, num_tabular_features, cnn_output_dim=256):\n",
        "        super().__init__()\n",
        "        self.cnn_branch = nn.Sequential(\n",
        "            nn.Conv3d(1, 32, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm3d(32),\n",
        "            nn.MaxPool3d(2, 2),\n",
        "\n",
        "            nn.Conv3d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm3d(64),\n",
        "            nn.MaxPool3d(2, 2),\n",
        "\n",
        "            nn.Conv3d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm3d(128),\n",
        "            nn.MaxPool3d(2, 2),\n",
        "\n",
        "            nn.Conv3d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm3d(256),\n",
        "            nn.MaxPool3d(2, 2),\n",
        "\n",
        "            nn.AdaptiveAvgPool3d((1, 1, 1)),\n",
        "            nn.Flatten())\n",
        "\n",
        "\n",
        "        # Note: self.tabular_branch outputs 1 feature, this is its linear transformation\n",
        "        self.tabular_branch = nn.Linear(num_tabular_features, 1)\n",
        "\n",
        "        self.image_head = nn.Sequential(\n",
        "            nn.Linear(cnn_output_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        self.cnn_output_dim = cnn_output_dim\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x_image, x_tabular):\n",
        "        image_features = self.cnn_branch(x_image)\n",
        "\n",
        "        tabular_risk = self.tabular_branch(x_tabular)\n",
        "        image_risk = self.image_head(image_features)\n",
        "        risk_score = image_risk + tabular_risk\n",
        "\n",
        "        return risk_score\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# Load imaging data\n",
        "imaging_df = pooled\n",
        "coord_cols = ['Xb', 'Yb', 'Zb']\n",
        "x_coords_all, y_coords_all, z_coords_all = imaging_df['Xb'].values, imaging_df['Yb'].values, imaging_df['Zb'].values\n",
        "x_unique_sorted, y_unique_sorted, z_unique_sorted = np.sort(np.unique(x_coords_all)), np.sort(np.unique(y_coords_all)), np.sort(np.unique(z_coords_all))\n",
        "dim_x, dim_y, dim_z = len(x_unique_sorted), len(y_unique_sorted), len(z_unique_sorted)\n",
        "print(f\"3D dimension: (X: {dim_x}, Y: {dim_y}, Z: {dim_z})\")\n",
        "\n",
        "# Create coordinate-to-index maps\n",
        "x_map, y_map, z_map = {val: i for i, val in enumerate(x_unique_sorted)}, {val: i for i, val in enumerate(y_unique_sorted)}, {val: i for i, val in enumerate(z_unique_sorted)}\n",
        "x_indices, y_indices, z_indices = np.array([x_map[val] for val in x_coords_all]), np.array([y_map[val] for val in y_coords_all]), np.array([z_map[val] for val in z_coords_all])\n",
        "\n",
        "# Reshape voxel data into 3D images\n",
        "patient_id_cols_from_img = imaging_df.columns[3:]\n",
        "num_patients = len(patient_id_cols_from_img)\n",
        "all_patient_images = np.zeros((num_patients, dim_x, dim_y, dim_z), dtype=np.float32)\n",
        "\n",
        "for i, patient_col_name in enumerate(patient_id_cols_from_img):\n",
        "    all_patient_images[i, x_indices, y_indices, z_indices] = imaging_df[patient_col_name].values\n",
        "\n",
        "# Add channel dimension for PyTorch -> (N, 1, X, Y, Z)\n",
        "X_data_images = np.expand_dims(all_patient_images, axis=1)\n",
        "print(f\"Image shape: {X_data_images.shape}\")\n",
        "\n",
        "# Create a mapping DataFrame from the image data\n",
        "cleaned_patient_ids = [pid.split('.')[0] for pid in patient_id_cols_from_img]\n",
        "\n",
        "imaging_ids_df = pd.DataFrame({\n",
        "    'imaging_patient_id': cleaned_patient_ids,\n",
        "    PATIENT_ID_COL: np.arange(1, num_patients + 1),\n",
        "    'image_idx': np.arange(num_patients)\n",
        "})\n",
        "\n",
        "## 2. Survival/Tabular Data Loading and Merging\n",
        "print(\"\\n--- Loading survival data ---\")\n",
        "survival_df = pd.read_csv(SURVIVAL_DF_PATH)\n",
        "df_final = pd.merge(survival_df, imaging_ids_df, on=PATIENT_ID_COL, how='inner')\n",
        "\n",
        "print(f\"Total patients: {len(df_final)}\")\n",
        "print(f\"Event rate: {df_final[EVENT_COL].mean():.2%}\")\n",
        "print(f\"Min follow-up time: {df_final[TIME_COL].min():.2f}\")\n",
        "print(f\"Max follow-up time: {df_final[TIME_COL].max():.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bVQGH2PnVaM",
        "outputId": "8a4ef217-e8c7-49da-8959-066f3d28b7ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3D dimension: (X: 77, Y: 92, Z: 74)\n",
            "Image shape: (742, 1, 77, 92, 74)\n",
            "\n",
            "--- Loading survival data ---\n",
            "Total patients: 742\n",
            "Event rate: 38.01%\n",
            "Min follow-up time: 0.00\n",
            "Max follow-up time: 13.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CoxPHloss(nn.Module):\n",
        "    def __init__(self, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, risk_scores, times, events):\n",
        "        # 1. sort\n",
        "        idx = torch.argsort(times)\n",
        "        rs  = risk_scores[idx]\n",
        "        e   = events[idx].float()\n",
        "\n",
        "        # 2. log Σ_{t_j ≥ t_i} exp(η_j)\n",
        "        log_risk_set = torch.logcumsumexp(rs.flip(0), 0).flip(0)\n",
        "\n",
        "        # 3. -(η_i - log Σ) * event\n",
        "        neg_log_pl = (log_risk_set - rs) * e\n",
        "        loss = neg_log_pl.sum()\n",
        "\n",
        "        # 4. reduction\n",
        "        if self.reduction == \"mean\":\n",
        "            num_events = e.sum()\n",
        "            if num_events > 0:\n",
        "                loss = loss / num_events\n",
        "            else:  # 全删失\n",
        "                return torch.tensor(0., dtype=rs.dtype, device=rs.device)\n",
        "\n",
        "        return loss\n",
        "class SurvivalDataset(Dataset):\n",
        "    def __init__(self, df, image_data, tabular_cols, time_col, event_col):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_data = image_data\n",
        "        self.tabular_cols = tabular_cols\n",
        "        self.time_col = time_col\n",
        "        self.event_col = event_col\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Get image\n",
        "        image_idx = row['image_idx']\n",
        "        image = self.image_data[image_idx]\n",
        "\n",
        "        # Get tabular features\n",
        "        tabular = row[self.tabular_cols].values.astype(np.float32)\n",
        "\n",
        "        # Get survival outcomes\n",
        "        time = row[self.time_col]\n",
        "        event = row[self.event_col]\n",
        "\n",
        "        return {\n",
        "            'image': torch.tensor(image, dtype=torch.float32),\n",
        "            'tabular': torch.tensor(tabular, dtype=torch.float32),\n",
        "            'time': torch.tensor(time, dtype=torch.float32),\n",
        "            'event': torch.tensor(event, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "## 5. Training Function\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, l1_lambda=1e-4):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        images = batch['image'].to(device)\n",
        "        tabular = batch['tabular'].to(device)\n",
        "        times = batch['time'].to(device)\n",
        "        events = batch['event'].to(device)\n",
        "        risk_scores = model(images, tabular).squeeze()\n",
        "\n",
        "        loss = criterion(risk_scores, times, events)\n",
        "\n",
        "        # L1 norm\n",
        "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "        total_batch_loss = loss + l1_lambda * l1_norm\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += total_batch_loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "## 6. Evaluation Function\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_risks = []\n",
        "    all_times = []\n",
        "    all_events = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images = batch['image'].to(device)\n",
        "            tabular = batch['tabular'].to(device)\n",
        "            times = batch['time']\n",
        "            events = batch['event']\n",
        "\n",
        "            risk_scores = model(images, tabular).squeeze()\n",
        "\n",
        "            all_risks.extend(risk_scores.cpu().numpy())\n",
        "            all_times.extend(times.numpy())\n",
        "            all_events.extend(events.numpy())\n",
        "\n",
        "    # Calculate C-index\n",
        "    c_index = concordance_index(all_times, -np.array(all_risks), all_events)\n",
        "\n",
        "    return c_index, all_risks, all_times, all_events\n",
        "\n"
      ],
      "metadata": {
        "id": "d1tkkFH0q13X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_5fold_cv_simple(df_final, X_data_images, tabular_cols, time_col, event_col,\n",
        "                        n_epochs=100, batch_size=32, learning_rate=0.001, device=DEVICE):\n",
        "\n",
        "    # Initialize results storage without train_loss\n",
        "    cv_results = {\n",
        "        'fold': [],\n",
        "        'train_c_index': [],\n",
        "        'val_c_index': []\n",
        "    }\n",
        "\n",
        "    # Prepare stratified k-fold\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "    # Run 5-fold CV\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_final, df_final[event_col])):\n",
        "        print(f\"\\n--- Fold {fold + 1}/5 ---\")\n",
        "\n",
        "        # Split data\n",
        "        train_df = df_final.iloc[train_idx].copy()\n",
        "        val_df = df_final.iloc[val_idx].copy()\n",
        "\n",
        "        # Standardize tabular features\n",
        "        scaler = StandardScaler()\n",
        "        train_df[tabular_cols] = scaler.fit_transform(train_df[tabular_cols])\n",
        "        val_df[tabular_cols] = scaler.transform(val_df[tabular_cols])\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = SurvivalDataset(train_df, X_data_images, tabular_cols, time_col, event_col)\n",
        "        val_dataset = SurvivalDataset(val_df, X_data_images, tabular_cols, time_col, event_col)\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Initialize model\n",
        "        model = SurvivalCNN(num_tabular_features=len(tabular_cols)).to(device)\n",
        "\n",
        "        # Loss and optimizer\n",
        "        criterion = CoxPHloss()\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=30, factor=0.5,min_lr=1e-8,verbose=True)\n",
        "\n",
        "        # Training loop\n",
        "        best_val_c_index = 0\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Train\n",
        "            train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "            # Evaluate\n",
        "            train_c_index, _, _, _ = evaluate_model(model, train_loader, device)\n",
        "            val_c_index, _, _, _ = evaluate_model(model, val_loader, device)\n",
        "\n",
        "            # Update scheduler\n",
        "            scheduler.step(-val_c_index)\n",
        "\n",
        "            # Save best model\n",
        "            if val_c_index > best_val_c_index:\n",
        "                best_val_c_index = val_c_index\n",
        "                torch.save(model.state_dict(), f'best_model_fold_{fold}.pth')\n",
        "\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{n_epochs}: \"\n",
        "                      f\"Train Loss: {train_loss:.4f}, \"\n",
        "                      f\"Train C-index: {train_c_index:.4f}, \"\n",
        "                      f\"Val C-index: {val_c_index:.4f}\")\n",
        "\n",
        "        # Load best model and get final results\n",
        "        model.load_state_dict(torch.load(f'best_model_fold_{fold}.pth'))\n",
        "        final_train_c_index, _, _, _ = evaluate_model(model, train_loader, device)\n",
        "        final_val_c_index, _, _, _ = evaluate_model(model, val_loader, device)\n",
        "\n",
        "        # Store results\n",
        "        cv_results['fold'].append(fold + 1)\n",
        "        cv_results['train_c_index'].append(final_train_c_index)\n",
        "        cv_results['val_c_index'].append(final_val_c_index)\n",
        "\n",
        "        print(f\"Fold {fold + 1} - Best Val C-index: {final_val_c_index:.4f}\")\n",
        "        print(f\"Fold {fold + 1} - Train C-index at best: {final_train_c_index:.4f}\")\n",
        "\n",
        "    # Calculate average results\n",
        "    avg_train_c_index = np.mean(cv_results['train_c_index'])\n",
        "    std_train_c_index = np.std(cv_results['train_c_index'])\n",
        "    avg_val_c_index = np.mean(cv_results['val_c_index'])\n",
        "    std_val_c_index = np.std(cv_results['val_c_index'])\n",
        "\n",
        "    print(f\"\\n--- 5-Fold CV Results ---\")\n",
        "    print(f\"Average Train C-index: {avg_train_c_index:.4f} ± {std_train_c_index:.4f}\")\n",
        "    print(f\"Average Val C-index: {avg_val_c_index:.4f} ± {std_val_c_index:.4f}\")\n",
        "\n",
        "    # Display fold-wise results\n",
        "    results_df = pd.DataFrame(cv_results)\n",
        "    print(\"\\nDetailed Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "    return cv_results, results_df"
      ],
      "metadata": {
        "id": "dlUUxSsrMp6z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_results, results_df = run_5fold_cv_simple(\n",
        "        df_final=df_final,\n",
        "        X_data_images=X_data_images,\n",
        "        tabular_cols=TABULAR_COLS,\n",
        "        time_col=TIME_COL,\n",
        "        event_col=EVENT_COL,\n",
        "        n_epochs=200,\n",
        "        batch_size=80,\n",
        "        learning_rate=0.001,\n",
        "        device=DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-XMsNNxMybM",
        "outputId": "0a227329-f6d4-4e31-ee88-ac9203d8520d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n",
            "Epoch 5/200: Train Loss: 7.2042, Train C-index: 0.5981, Val C-index: 0.5837\n",
            "Epoch 10/200: Train Loss: 6.5666, Train C-index: 0.6236, Val C-index: 0.5821\n",
            "Epoch 15/200: Train Loss: 6.3184, Train C-index: 0.6661, Val C-index: 0.6031\n",
            "Epoch 20/200: Train Loss: 5.8653, Train C-index: 0.6890, Val C-index: 0.6049\n",
            "Epoch 25/200: Train Loss: 5.3171, Train C-index: 0.7635, Val C-index: 0.6210\n",
            "Epoch 30/200: Train Loss: 5.1422, Train C-index: 0.8017, Val C-index: 0.6638\n",
            "Epoch 35/200: Train Loss: 4.6860, Train C-index: 0.7125, Val C-index: 0.6487\n",
            "Epoch 40/200: Train Loss: 4.6109, Train C-index: 0.8054, Val C-index: 0.6473\n",
            "Epoch 45/200: Train Loss: 4.7595, Train C-index: 0.8256, Val C-index: 0.6691\n",
            "Epoch 50/200: Train Loss: 4.1343, Train C-index: 0.9312, Val C-index: 0.7103\n",
            "Epoch 55/200: Train Loss: 3.8397, Train C-index: 0.9117, Val C-index: 0.6764\n",
            "Epoch 60/200: Train Loss: 3.6579, Train C-index: 0.9373, Val C-index: 0.7022\n",
            "Epoch 65/200: Train Loss: 3.6362, Train C-index: 0.9351, Val C-index: 0.6939\n",
            "Epoch 70/200: Train Loss: 3.2949, Train C-index: 0.9599, Val C-index: 0.7066\n",
            "Epoch 75/200: Train Loss: 3.2562, Train C-index: 0.9646, Val C-index: 0.6911\n",
            "Epoch 80/200: Train Loss: 3.2063, Train C-index: 0.9680, Val C-index: 0.7179\n",
            "Epoch 85/200: Train Loss: 3.2103, Train C-index: 0.9656, Val C-index: 0.7129\n",
            "Epoch 90/200: Train Loss: 3.2533, Train C-index: 0.9653, Val C-index: 0.7012\n",
            "Epoch 95/200: Train Loss: 2.9643, Train C-index: 0.9680, Val C-index: 0.7135\n",
            "Epoch 100/200: Train Loss: 2.9670, Train C-index: 0.9735, Val C-index: 0.6992\n",
            "Epoch 105/200: Train Loss: 2.8216, Train C-index: 0.9713, Val C-index: 0.7058\n",
            "Epoch 110/200: Train Loss: 2.6617, Train C-index: 0.9705, Val C-index: 0.7072\n",
            "Epoch 115/200: Train Loss: 2.9188, Train C-index: 0.9606, Val C-index: 0.6852\n",
            "Epoch 120/200: Train Loss: 2.5686, Train C-index: 0.9674, Val C-index: 0.6919\n",
            "Epoch 125/200: Train Loss: 2.3353, Train C-index: 0.9723, Val C-index: 0.7004\n",
            "Epoch 130/200: Train Loss: 2.5244, Train C-index: 0.9814, Val C-index: 0.7089\n",
            "Epoch 135/200: Train Loss: 2.4111, Train C-index: 0.9837, Val C-index: 0.6984\n",
            "Epoch 140/200: Train Loss: 2.2870, Train C-index: 0.9809, Val C-index: 0.6863\n",
            "Epoch 145/200: Train Loss: 2.2941, Train C-index: 0.9850, Val C-index: 0.6933\n",
            "Epoch 150/200: Train Loss: 2.3438, Train C-index: 0.9841, Val C-index: 0.6897\n",
            "Epoch 155/200: Train Loss: 2.1464, Train C-index: 0.9893, Val C-index: 0.7040\n",
            "Epoch 160/200: Train Loss: 2.1334, Train C-index: 0.9881, Val C-index: 0.6976\n",
            "Epoch 165/200: Train Loss: 2.0123, Train C-index: 0.9893, Val C-index: 0.6978\n",
            "Epoch 170/200: Train Loss: 2.2674, Train C-index: 0.9864, Val C-index: 0.7072\n",
            "Epoch 175/200: Train Loss: 2.1162, Train C-index: 0.9887, Val C-index: 0.6967\n",
            "Epoch 180/200: Train Loss: 2.0036, Train C-index: 0.9899, Val C-index: 0.7056\n",
            "Epoch 185/200: Train Loss: 1.9571, Train C-index: 0.9909, Val C-index: 0.7012\n",
            "Epoch 190/200: Train Loss: 1.9109, Train C-index: 0.9932, Val C-index: 0.6957\n",
            "Epoch 195/200: Train Loss: 1.8841, Train C-index: 0.9936, Val C-index: 0.6970\n",
            "Epoch 200/200: Train Loss: 1.8550, Train C-index: 0.9941, Val C-index: 0.6941\n",
            "Fold 1 - Best Val C-index: 0.7262\n",
            "Fold 1 - Train C-index at best: 0.9652\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Epoch 5/200: Train Loss: 7.0771, Train C-index: 0.5906, Val C-index: 0.6190\n",
            "Epoch 10/200: Train Loss: 6.5340, Train C-index: 0.6298, Val C-index: 0.6216\n",
            "Epoch 15/200: Train Loss: 6.3633, Train C-index: 0.6491, Val C-index: 0.6490\n",
            "Epoch 20/200: Train Loss: 6.1461, Train C-index: 0.6551, Val C-index: 0.6583\n",
            "Epoch 25/200: Train Loss: 5.5751, Train C-index: 0.6655, Val C-index: 0.6573\n",
            "Epoch 30/200: Train Loss: 5.2109, Train C-index: 0.7085, Val C-index: 0.6729\n",
            "Epoch 35/200: Train Loss: 4.7874, Train C-index: 0.7066, Val C-index: 0.6266\n",
            "Epoch 40/200: Train Loss: 4.7037, Train C-index: 0.7626, Val C-index: 0.6565\n",
            "Epoch 45/200: Train Loss: 4.3815, Train C-index: 0.8137, Val C-index: 0.6627\n",
            "Epoch 50/200: Train Loss: 3.8824, Train C-index: 0.7878, Val C-index: 0.6400\n",
            "Epoch 55/200: Train Loss: 3.5733, Train C-index: 0.8593, Val C-index: 0.6312\n",
            "Epoch 60/200: Train Loss: 3.8374, Train C-index: 0.8819, Val C-index: 0.6398\n",
            "Epoch 65/200: Train Loss: 3.6947, Train C-index: 0.6348, Val C-index: 0.5381\n",
            "Epoch 70/200: Train Loss: 3.8353, Train C-index: 0.8297, Val C-index: 0.6504\n",
            "Epoch 75/200: Train Loss: 3.4870, Train C-index: 0.9189, Val C-index: 0.6767\n",
            "Epoch 80/200: Train Loss: 3.1028, Train C-index: 0.9477, Val C-index: 0.6452\n",
            "Epoch 85/200: Train Loss: 3.2124, Train C-index: 0.9054, Val C-index: 0.6931\n",
            "Epoch 90/200: Train Loss: 2.8001, Train C-index: 0.9624, Val C-index: 0.6372\n",
            "Epoch 95/200: Train Loss: 3.2639, Train C-index: 0.9561, Val C-index: 0.6506\n",
            "Epoch 100/200: Train Loss: 2.8732, Train C-index: 0.9683, Val C-index: 0.6424\n",
            "Epoch 105/200: Train Loss: 2.7607, Train C-index: 0.9680, Val C-index: 0.6579\n",
            "Epoch 110/200: Train Loss: 2.7060, Train C-index: 0.9710, Val C-index: 0.6585\n",
            "Epoch 115/200: Train Loss: 2.7166, Train C-index: 0.9704, Val C-index: 0.6560\n",
            "Epoch 120/200: Train Loss: 2.5510, Train C-index: 0.9777, Val C-index: 0.6490\n",
            "Epoch 125/200: Train Loss: 2.3754, Train C-index: 0.9793, Val C-index: 0.6661\n",
            "Epoch 130/200: Train Loss: 2.2342, Train C-index: 0.9756, Val C-index: 0.6691\n",
            "Epoch 135/200: Train Loss: 2.3071, Train C-index: 0.9780, Val C-index: 0.6699\n",
            "Epoch 140/200: Train Loss: 2.3192, Train C-index: 0.9811, Val C-index: 0.6823\n",
            "Epoch 145/200: Train Loss: 2.3577, Train C-index: 0.9836, Val C-index: 0.6633\n",
            "Epoch 150/200: Train Loss: 2.0595, Train C-index: 0.9875, Val C-index: 0.6512\n",
            "Epoch 155/200: Train Loss: 1.9803, Train C-index: 0.9881, Val C-index: 0.6713\n",
            "Epoch 160/200: Train Loss: 1.9681, Train C-index: 0.9885, Val C-index: 0.6765\n",
            "Epoch 165/200: Train Loss: 1.9176, Train C-index: 0.9876, Val C-index: 0.6723\n",
            "Epoch 170/200: Train Loss: 1.9356, Train C-index: 0.9899, Val C-index: 0.6595\n",
            "Epoch 175/200: Train Loss: 1.9291, Train C-index: 0.9888, Val C-index: 0.6496\n",
            "Epoch 180/200: Train Loss: 1.8195, Train C-index: 0.9903, Val C-index: 0.6546\n",
            "Epoch 185/200: Train Loss: 1.8764, Train C-index: 0.9919, Val C-index: 0.6575\n",
            "Epoch 190/200: Train Loss: 1.8622, Train C-index: 0.9916, Val C-index: 0.6619\n",
            "Epoch 195/200: Train Loss: 1.8094, Train C-index: 0.9927, Val C-index: 0.6623\n",
            "Epoch 200/200: Train Loss: 1.8244, Train C-index: 0.9940, Val C-index: 0.6637\n",
            "Fold 2 - Best Val C-index: 0.6931\n",
            "Fold 2 - Train C-index at best: 0.9054\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Epoch 5/200: Train Loss: 6.7061, Train C-index: 0.5630, Val C-index: 0.6193\n",
            "Epoch 10/200: Train Loss: 6.4029, Train C-index: 0.5834, Val C-index: 0.6225\n",
            "Epoch 15/200: Train Loss: 6.1514, Train C-index: 0.6051, Val C-index: 0.6461\n",
            "Epoch 20/200: Train Loss: 5.7734, Train C-index: 0.6260, Val C-index: 0.6465\n",
            "Epoch 25/200: Train Loss: 5.4457, Train C-index: 0.6116, Val C-index: 0.6165\n",
            "Epoch 30/200: Train Loss: 4.9949, Train C-index: 0.7050, Val C-index: 0.7115\n",
            "Epoch 35/200: Train Loss: 5.2483, Train C-index: 0.6338, Val C-index: 0.6227\n",
            "Epoch 40/200: Train Loss: 4.6896, Train C-index: 0.6426, Val C-index: 0.5848\n",
            "Epoch 45/200: Train Loss: 4.4951, Train C-index: 0.8504, Val C-index: 0.6839\n",
            "Epoch 50/200: Train Loss: 3.9114, Train C-index: 0.9367, Val C-index: 0.6961\n",
            "Epoch 55/200: Train Loss: 3.7343, Train C-index: 0.8849, Val C-index: 0.6880\n",
            "Epoch 60/200: Train Loss: 3.5296, Train C-index: 0.8712, Val C-index: 0.6582\n",
            "Epoch 65/200: Train Loss: 3.5397, Train C-index: 0.8963, Val C-index: 0.7000\n",
            "Epoch 70/200: Train Loss: 3.2703, Train C-index: 0.9468, Val C-index: 0.6354\n",
            "Epoch 75/200: Train Loss: 2.9027, Train C-index: 0.9751, Val C-index: 0.6615\n",
            "Epoch 80/200: Train Loss: 2.8430, Train C-index: 0.9767, Val C-index: 0.6548\n",
            "Epoch 85/200: Train Loss: 2.7013, Train C-index: 0.9434, Val C-index: 0.6341\n",
            "Epoch 90/200: Train Loss: 2.6889, Train C-index: 0.9681, Val C-index: 0.6403\n",
            "Epoch 95/200: Train Loss: 2.6990, Train C-index: 0.9643, Val C-index: 0.6829\n",
            "Epoch 100/200: Train Loss: 2.6873, Train C-index: 0.9706, Val C-index: 0.6621\n",
            "Epoch 105/200: Train Loss: 2.4289, Train C-index: 0.9819, Val C-index: 0.6627\n",
            "Epoch 110/200: Train Loss: 2.4074, Train C-index: 0.9843, Val C-index: 0.6595\n",
            "Epoch 115/200: Train Loss: 2.3416, Train C-index: 0.9782, Val C-index: 0.6388\n",
            "Epoch 120/200: Train Loss: 2.4285, Train C-index: 0.9870, Val C-index: 0.6610\n",
            "Epoch 125/200: Train Loss: 2.2510, Train C-index: 0.9788, Val C-index: 0.6642\n",
            "Epoch 130/200: Train Loss: 2.3495, Train C-index: 0.9869, Val C-index: 0.6533\n",
            "Epoch 135/200: Train Loss: 2.2181, Train C-index: 0.9894, Val C-index: 0.6493\n",
            "Epoch 140/200: Train Loss: 2.2505, Train C-index: 0.9902, Val C-index: 0.6553\n",
            "Epoch 145/200: Train Loss: 2.0310, Train C-index: 0.9905, Val C-index: 0.6531\n",
            "Epoch 150/200: Train Loss: 2.2145, Train C-index: 0.9902, Val C-index: 0.6495\n",
            "Epoch 155/200: Train Loss: 2.0378, Train C-index: 0.9914, Val C-index: 0.6493\n",
            "Epoch 160/200: Train Loss: 2.0022, Train C-index: 0.9910, Val C-index: 0.6565\n",
            "Epoch 165/200: Train Loss: 2.0979, Train C-index: 0.9928, Val C-index: 0.6465\n",
            "Epoch 170/200: Train Loss: 2.0191, Train C-index: 0.9928, Val C-index: 0.6480\n",
            "Epoch 175/200: Train Loss: 1.9824, Train C-index: 0.9935, Val C-index: 0.6467\n",
            "Epoch 180/200: Train Loss: 1.9653, Train C-index: 0.9937, Val C-index: 0.6448\n",
            "Epoch 185/200: Train Loss: 2.0106, Train C-index: 0.9934, Val C-index: 0.6510\n",
            "Epoch 190/200: Train Loss: 1.8732, Train C-index: 0.9942, Val C-index: 0.6446\n",
            "Epoch 195/200: Train Loss: 1.9758, Train C-index: 0.9943, Val C-index: 0.6529\n",
            "Epoch 200/200: Train Loss: 2.0383, Train C-index: 0.9949, Val C-index: 0.6488\n",
            "Fold 3 - Best Val C-index: 0.7220\n",
            "Fold 3 - Train C-index at best: 0.7580\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Epoch 5/200: Train Loss: 7.1243, Train C-index: 0.4814, Val C-index: 0.5875\n",
            "Epoch 10/200: Train Loss: 6.7251, Train C-index: 0.5513, Val C-index: 0.6520\n",
            "Epoch 15/200: Train Loss: 6.3441, Train C-index: 0.6101, Val C-index: 0.6843\n",
            "Epoch 20/200: Train Loss: 6.1121, Train C-index: 0.6536, Val C-index: 0.6918\n",
            "Epoch 25/200: Train Loss: 5.6123, Train C-index: 0.7124, Val C-index: 0.6687\n",
            "Epoch 30/200: Train Loss: 5.0644, Train C-index: 0.7710, Val C-index: 0.6957\n",
            "Epoch 35/200: Train Loss: 4.7773, Train C-index: 0.6581, Val C-index: 0.6344\n",
            "Epoch 40/200: Train Loss: 4.5630, Train C-index: 0.8899, Val C-index: 0.6824\n",
            "Epoch 45/200: Train Loss: 4.0972, Train C-index: 0.9347, Val C-index: 0.6749\n",
            "Epoch 50/200: Train Loss: 4.0096, Train C-index: 0.9120, Val C-index: 0.6909\n",
            "Epoch 55/200: Train Loss: 3.8520, Train C-index: 0.9228, Val C-index: 0.6614\n",
            "Epoch 60/200: Train Loss: 3.4845, Train C-index: 0.9428, Val C-index: 0.6670\n",
            "Epoch 65/200: Train Loss: 3.1273, Train C-index: 0.9648, Val C-index: 0.6907\n",
            "Epoch 70/200: Train Loss: 3.1211, Train C-index: 0.9733, Val C-index: 0.6890\n",
            "Epoch 75/200: Train Loss: 2.9748, Train C-index: 0.9697, Val C-index: 0.6792\n",
            "Epoch 80/200: Train Loss: 3.0042, Train C-index: 0.9747, Val C-index: 0.6867\n",
            "Epoch 85/200: Train Loss: 2.9201, Train C-index: 0.9217, Val C-index: 0.6663\n",
            "Epoch 90/200: Train Loss: 2.6395, Train C-index: 0.9700, Val C-index: 0.6834\n",
            "Epoch 95/200: Train Loss: 2.6185, Train C-index: 0.9835, Val C-index: 0.6897\n",
            "Epoch 100/200: Train Loss: 2.4572, Train C-index: 0.9872, Val C-index: 0.6815\n",
            "Epoch 105/200: Train Loss: 2.3799, Train C-index: 0.9878, Val C-index: 0.6849\n",
            "Epoch 110/200: Train Loss: 2.5212, Train C-index: 0.9853, Val C-index: 0.6794\n",
            "Epoch 115/200: Train Loss: 2.4077, Train C-index: 0.9843, Val C-index: 0.6830\n",
            "Epoch 120/200: Train Loss: 2.3086, Train C-index: 0.9869, Val C-index: 0.6886\n",
            "Epoch 125/200: Train Loss: 2.3135, Train C-index: 0.9888, Val C-index: 0.6813\n",
            "Epoch 130/200: Train Loss: 2.2195, Train C-index: 0.9904, Val C-index: 0.6830\n",
            "Epoch 135/200: Train Loss: 2.3179, Train C-index: 0.9883, Val C-index: 0.6832\n",
            "Epoch 140/200: Train Loss: 2.1763, Train C-index: 0.9922, Val C-index: 0.6779\n",
            "Epoch 145/200: Train Loss: 2.2056, Train C-index: 0.9923, Val C-index: 0.6815\n",
            "Epoch 150/200: Train Loss: 2.2856, Train C-index: 0.9923, Val C-index: 0.6839\n",
            "Epoch 155/200: Train Loss: 2.1692, Train C-index: 0.9922, Val C-index: 0.6830\n",
            "Epoch 160/200: Train Loss: 2.0178, Train C-index: 0.9929, Val C-index: 0.6841\n",
            "Epoch 165/200: Train Loss: 2.0306, Train C-index: 0.9929, Val C-index: 0.6813\n",
            "Epoch 170/200: Train Loss: 2.0698, Train C-index: 0.9924, Val C-index: 0.6777\n",
            "Epoch 175/200: Train Loss: 1.9844, Train C-index: 0.9942, Val C-index: 0.6813\n",
            "Epoch 180/200: Train Loss: 1.9978, Train C-index: 0.9932, Val C-index: 0.6813\n",
            "Epoch 185/200: Train Loss: 2.1636, Train C-index: 0.9928, Val C-index: 0.6813\n",
            "Epoch 190/200: Train Loss: 2.0170, Train C-index: 0.9951, Val C-index: 0.6822\n",
            "Epoch 195/200: Train Loss: 2.0293, Train C-index: 0.9958, Val C-index: 0.6834\n",
            "Epoch 200/200: Train Loss: 2.0231, Train C-index: 0.9950, Val C-index: 0.6830\n",
            "Fold 4 - Best Val C-index: 0.7149\n",
            "Fold 4 - Train C-index at best: 0.8709\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Epoch 5/200: Train Loss: 7.0906, Train C-index: 0.5693, Val C-index: 0.5505\n",
            "Epoch 10/200: Train Loss: 6.7337, Train C-index: 0.5786, Val C-index: 0.5618\n",
            "Epoch 15/200: Train Loss: 6.4690, Train C-index: 0.6041, Val C-index: 0.5580\n",
            "Epoch 20/200: Train Loss: 6.1157, Train C-index: 0.6096, Val C-index: 0.5591\n",
            "Epoch 25/200: Train Loss: 5.7523, Train C-index: 0.6361, Val C-index: 0.5727\n",
            "Epoch 30/200: Train Loss: 5.6301, Train C-index: 0.6639, Val C-index: 0.5735\n",
            "Epoch 35/200: Train Loss: 5.3309, Train C-index: 0.6827, Val C-index: 0.6365\n",
            "Epoch 40/200: Train Loss: 4.6473, Train C-index: 0.8153, Val C-index: 0.6281\n",
            "Epoch 45/200: Train Loss: 4.2878, Train C-index: 0.8282, Val C-index: 0.6394\n",
            "Epoch 50/200: Train Loss: 4.2061, Train C-index: 0.8410, Val C-index: 0.6499\n",
            "Epoch 55/200: Train Loss: 4.0155, Train C-index: 0.9032, Val C-index: 0.6457\n",
            "Epoch 60/200: Train Loss: 3.8211, Train C-index: 0.9384, Val C-index: 0.6111\n",
            "Epoch 65/200: Train Loss: 3.5329, Train C-index: 0.9401, Val C-index: 0.6348\n",
            "Epoch 70/200: Train Loss: 3.3416, Train C-index: 0.8995, Val C-index: 0.6484\n",
            "Epoch 75/200: Train Loss: 3.3872, Train C-index: 0.9481, Val C-index: 0.6316\n",
            "Epoch 80/200: Train Loss: 3.2597, Train C-index: 0.9174, Val C-index: 0.6058\n",
            "Epoch 85/200: Train Loss: 3.1104, Train C-index: 0.9558, Val C-index: 0.6371\n",
            "Epoch 90/200: Train Loss: 3.2117, Train C-index: 0.9201, Val C-index: 0.6073\n",
            "Epoch 95/200: Train Loss: 2.8451, Train C-index: 0.9663, Val C-index: 0.6547\n",
            "Epoch 100/200: Train Loss: 2.7122, Train C-index: 0.9733, Val C-index: 0.6214\n",
            "Epoch 105/200: Train Loss: 2.5138, Train C-index: 0.9779, Val C-index: 0.6591\n",
            "Epoch 110/200: Train Loss: 2.5446, Train C-index: 0.9649, Val C-index: 0.6386\n",
            "Epoch 115/200: Train Loss: 2.4828, Train C-index: 0.9827, Val C-index: 0.6335\n",
            "Epoch 120/200: Train Loss: 2.5210, Train C-index: 0.9716, Val C-index: 0.6178\n",
            "Epoch 125/200: Train Loss: 2.2947, Train C-index: 0.9803, Val C-index: 0.6178\n",
            "Epoch 130/200: Train Loss: 2.2467, Train C-index: 0.9859, Val C-index: 0.6344\n",
            "Epoch 135/200: Train Loss: 2.3249, Train C-index: 0.9875, Val C-index: 0.6226\n",
            "Epoch 140/200: Train Loss: 2.1567, Train C-index: 0.9828, Val C-index: 0.6308\n",
            "Epoch 145/200: Train Loss: 2.1275, Train C-index: 0.9839, Val C-index: 0.6327\n",
            "Epoch 150/200: Train Loss: 2.1910, Train C-index: 0.9884, Val C-index: 0.6241\n",
            "Epoch 155/200: Train Loss: 2.2549, Train C-index: 0.9830, Val C-index: 0.6235\n",
            "Epoch 160/200: Train Loss: 2.0479, Train C-index: 0.9874, Val C-index: 0.6251\n",
            "Epoch 165/200: Train Loss: 1.9223, Train C-index: 0.9911, Val C-index: 0.6226\n",
            "Epoch 170/200: Train Loss: 1.9792, Train C-index: 0.9930, Val C-index: 0.6356\n",
            "Epoch 175/200: Train Loss: 2.0001, Train C-index: 0.9865, Val C-index: 0.6237\n",
            "Epoch 180/200: Train Loss: 1.9274, Train C-index: 0.9900, Val C-index: 0.6279\n",
            "Epoch 185/200: Train Loss: 1.9624, Train C-index: 0.9920, Val C-index: 0.6354\n",
            "Epoch 190/200: Train Loss: 1.9134, Train C-index: 0.9933, Val C-index: 0.6268\n",
            "Epoch 195/200: Train Loss: 1.9283, Train C-index: 0.9945, Val C-index: 0.6295\n",
            "Epoch 200/200: Train Loss: 1.9398, Train C-index: 0.9949, Val C-index: 0.6272\n",
            "Fold 5 - Best Val C-index: 0.6650\n",
            "Fold 5 - Train C-index at best: 0.9072\n",
            "\n",
            "--- 5-Fold CV Results ---\n",
            "Average Train C-index: 0.8813 ± 0.0687\n",
            "Average Val C-index: 0.7043 ± 0.0227\n",
            "\n",
            "Detailed Results:\n",
            "   fold  train_c_index  val_c_index\n",
            "0     1       0.965190     0.726227\n",
            "1     2       0.905372     0.693091\n",
            "2     3       0.757962     0.722020\n",
            "3     4       0.870875     0.714928\n",
            "4     5       0.907197     0.664988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USBRIPFYOET6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-HBL489OEcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}